---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Narcissism survey analysis


**MB**: The data are from <https://openpsychometrics.org/_rawdata/>, specifically, the data for the Narcissistic Personality Inventory <http://openpsychometrics.org/_rawdata/NPI.zip>.

```{python}
import numpy as np
import pandas as pd
pd.set_option('mode.copy_on_write', True)
import matplotlib.pyplot as plt
```

```{python}
df = pd.read_csv('NPI/data.csv')
df.head()
```

## A partial correlation


Let's say we are interested in the scores from Q10.

We know and are not interested by the fact that the results from Q1 can partially predict the results of Q10.

What we want to know is, is whether Q2 predicts Q10, *after removing any effect that can be explained by Q1*.

We first use the code from the textbook to calculate correlations - see <https://lisds.github.io/textbook/mean-slopes/Correlation.html>.

```{python}
def standard_units(x):
    "Convert any array of numbers to standard units."
    return (x - np.mean(x)) / np.std(x)
```

```{python}
# A version of the function from the textbook.

def correlation(x_vals, y_vals):
    """ Correlation by calculation
    """
    return np.mean(standard_units(x_vals) * standard_units(y_vals))
```

As expected, there is a negative correlation between question 1 and question 10.

```{python}
r_q1_q10 = correlation(df['Q1'], df['Q10'])
r_q1_q10
```

Our question is - what if we *removed* this effect (of Q1) from the values of Q10?   Would Q2 be able to predict the remaining (residual) values of Q10?  You can see this approach in <https://lisds.github.io/textbook/classification/single_multiple.html#multiple-regression-in-steps>.

We first do a regression, predicting Q10 from Q1, following the recipe in the page above.  We'll use the least-squares shortcut to minimize, for simplicity (see the page above for details).

```{python}
from scipy.stats import linregress
```

```{python}
res = linregress(df['Q1'], df['Q10'])
res
```

Notice this is the regression corresponding to the correlation we did with our own function - you can see the previous r value in the output above.   With this regression, we have the slope and intercept to get the *predicted* values for Q10 from the values of Q1.

```{python}
q10_predicted = res.intercept + res.slope * df['Q1']
```

Now we can calculate the *residuals* for Q10.  The residuals are the values left over (residual) after subtracting the predictions we got from Q1.

```{python}
q10_residuals = df['Q10'] - q10_predicted
q10_residuals
```

The residuals are the values of Q10 *after removing our estimate of the effect of Q1*.   So now we can ask our question of interest - how well does Q2 predict Q10, *after removing the effect of Q1*?:

```{python}
correlation(df['Q2'], q10_residuals)
```

This is known as the *partial* correlation of Q2 with Q10, adjusting for Q1.


Of course, we could also get the correlation with `linregress`, as we did above.  In that way, we also get the slope and intercept.

```{python}
linregress(df['Q2'], q10_residuals)
```
